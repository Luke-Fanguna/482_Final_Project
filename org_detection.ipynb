{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "#This is a helper function for calculating transcript times\n",
    "def add_seconds(start_time, seconds_to_add):\n",
    "  # Parse the start time\n",
    "  time_obj = datetime.strptime(start_time, '%H:%M:%S')\n",
    "  # Add the seconds\n",
    "  new_time = time_obj + timedelta(seconds=seconds_to_add)\n",
    "  # Format the new time back to string\n",
    "  return new_time.strftime('%H:%M:%S')\n",
    "\n",
    "#This is where the unzipped corpus file is stored in Colab file storage\n",
    "CORPUS_FILE_PATH = 'DH2024_Corpus_Release/'\n",
    "\n",
    "#We have 4 state sets\n",
    "VALID_STATES=[\"CA\", \"FL\", \"NY\", \"TX\"]\n",
    "\n",
    "#Each state set has 9 csv files for each session year\n",
    "CSV_FILENAMES=['bills','committeeHearings', 'committeeRosters',\n",
    "               'committees', 'hearings', 'legislature',\n",
    "               'people','speeches','videos']\n",
    "\n",
    "#Load data from CSVs into a Python object to reference later\n",
    "#Input:\n",
    "#  Required: file_name (type:String) (Ex: speeches, bills, etc)\n",
    "#  Optional: states (type:List of Strings or None) (Ex: [\"CA\"], [\"FL,TX\"])\n",
    "#     -If not specified (states=None), function returns data for all states\n",
    "#  Optional: years (type:List of Ints or None) (Ex:[2018], [2015,2016])\n",
    "#     -If not specified (years=None), function returns data for all valid years\n",
    "#Output:\n",
    "#  Payload (Type: Dict) (Ex: {column_headers:['pid','cid','date'], rows:[[0,2,2018],[2,1,2018]]})\n",
    "def load_content(file_name, states=None, years=None):\n",
    "  #Only accept valid states, Corpus only contains data on CA, FL, NY, and TX legislations\n",
    "  if states is not None and not all(item in VALID_STATES for item in states):\n",
    "    raise Exception(\"Invalid State Abbv(s), corpus only contains data on CA, FL, NY, and TX\")\n",
    "  #Only accept valid file names from corpus, like speeches, bills, etc.\n",
    "  if file_name not in CSV_FILENAMES:\n",
    "    raise Exception(\"Invalid filename, must be one of the 9 files provide\")\n",
    "  #Only accept years belonging to a valid legislative session. (2017-2018 for all states, 2015-2016 for CA)\n",
    "  if years is not None and ((not all(item > 2015 for item in years) and \"CA\" not in states) or (not all(item <= 2018 for item in years))):\n",
    "    raise Exception(\"\"\"Data for requested year not included in corpus.\n",
    "     Valid session_years are 2017 and 2018 for all states provided. 2015 and 2016 are valid years for CA.\"\"\")\n",
    "\n",
    "  payload = {}\n",
    "  header_row = True\n",
    "\n",
    "  #If no states specified, retrieve relevant files for all valid states\n",
    "  if states is None:\n",
    "    states = VALID_STATES\n",
    "\n",
    "  #If no years/session specified, retrieve data for all valid state legislative session years\n",
    "  if years is None:\n",
    "    if \"CA\" in states:\n",
    "      years= [2015,2016,2017,2018]\n",
    "    else:\n",
    "      years = [2017,2018]\n",
    "\n",
    "  #The following code block operates as follows:\n",
    "  # For every state and year requested, read the relevant CSV file(s), then\n",
    "  # load it into a python object (payload) which is returned to user\n",
    "  for state in states:\n",
    "    FILE_PATHS = []\n",
    "\n",
    "    #Build the filepaths to the correct data location given the states and years provided\n",
    "    #Years 2017 and 2018 are valid inputs that belong to the same 2017-2018 session\n",
    "    if 2017 in years or 2018 in years:\n",
    "      FILE_PATHS.append(CORPUS_FILE_PATH + state + \"/2017-2018/CSV/\" + file_name + \".csv\")\n",
    "\n",
    "    #CA has 2 valid legislative sessions (2015-2016 and 2017-2018)\n",
    "    #This means the entirety of CA data is located in more than one folder, unlike other states.\n",
    "    #Looping through a list of filepaths allows us to handle this corner case\n",
    "    if state == \"CA\" and (2015 in years or 2016 in years):\n",
    "      FILE_PATHS.append(CORPUS_FILE_PATH + state + \"/2015-2016/CSV/\" + file_name + \".csv\")\n",
    "\n",
    "    for FILE_PATH in FILE_PATHS:\n",
    "      #Open the file to read\n",
    "      with open(FILE_PATH, newline='', encoding='utf-8') as csvfile:\n",
    "        rows = csv.reader(csvfile, delimiter=',')\n",
    "        #Read CSV row by row\n",
    "        for row in rows:\n",
    "          #The first row of every CSV we visit is the header row, containing the names for each column\n",
    "          # We will add this to the payload only once, as every CSV we read after this will be the same headers\n",
    "          if header_row:\n",
    "            payload['column_headers'] = row\n",
    "            #Sets up 'rows' in payload where we will store future records\n",
    "            payload['rows'] = []\n",
    "            header_row = False\n",
    "            continue\n",
    "          #Load CSV Into payload row by row\n",
    "          payload['rows'].append(row)\n",
    "\n",
    "  return payload\n",
    "\n",
    "#Retrieve Committee Name & ID, hearing date, list of videos of the hearing, and state for a given HID\n",
    "#Input:\n",
    "#  Required: HID (type:Positive Int) (Ex: 10003)\n",
    "#  Optional: speeches\n",
    "#      -If searching through from a specific state or session, pass in\n",
    "#         speeches=load_content(\"speeches\", specific states, specific years)\n",
    "#Output:\n",
    "#   Dictionary\n",
    "#   If no matches exist does not exist in database, returns []\n",
    "def get_metadata_hearing(hid, hearings=None, videos=None):\n",
    "  HID_IDX=0\n",
    "  CID_IDX=4\n",
    "  CNAME_IDX=8\n",
    "  HDATE_IDX=1\n",
    "  STATE_IDX=3\n",
    "\n",
    "  if hearings is None:\n",
    "    hearings=load_content(\"hearings\")\n",
    "  hid = str(hid)\n",
    "\n",
    "  hearingData=None\n",
    "  for row in hearings['rows']:\n",
    "    if hid == row[HID_IDX]:\n",
    "      hearingData={'hid':row[HID_IDX],'cid':row[CID_IDX],'cname':row[CNAME_IDX],'hearing_date':row[HDATE_IDX],'state':row[STATE_IDX]}\n",
    "  if hearingData is None:\n",
    "    return {}\n",
    "\n",
    "  #vids, videos = videos_from_hid(hid)\n",
    "  #hearingData['vids'] = vids\n",
    "  hearingData['videos'] = videos_from_hid(hid)\n",
    "\n",
    "  return hearingData\n",
    "\n",
    "\n",
    "#Retrieve the video ids and video URLs associated with a hearing\n",
    "#Input:\n",
    "#  Required: Hearing ID (type:Positive Int) (Ex: 10003)\n",
    "#  Optional: videos\n",
    "#      -If searching through from a specific state or session, pass in\n",
    "#         videos=load_content(\"videos\", specific states, specific years)\n",
    "#Output:\n",
    "#   (Type: List of[[Int,String]]) (Ex: [[0, \"video0.mp4\"], [1,\"video1.mp4\"]])\n",
    "#   If no matches exist does not exist in database, returns []\n",
    "def videos_from_hid(hid, videos=None):\n",
    "  HID_IDX = 2\n",
    "  VID_IDX = 0\n",
    "  URL_IDX = 7\n",
    "\n",
    "  if videos is None:\n",
    "    videos=load_content(\"videos\")\n",
    "\n",
    "  vids = []\n",
    "  video_files=[]\n",
    "  hid=str(hid)\n",
    "\n",
    "  for row in videos['rows']:\n",
    "    if row[HID_IDX] == hid:\n",
    "      vids.append((row[VID_IDX],row[URL_IDX])) #tuple of (video id, url)\n",
    "  return vids\n",
    "\n",
    "#Create transcript for a given bill discussion with metadata\n",
    "#Input:\n",
    "#  Required: HID (type:Positive Int) (Ex: 10003)\n",
    "#  Required: BID (type: String)\n",
    "#  Optional: speeches\n",
    "#      -If searching through from a specific state or session, pass in\n",
    "#         speeches=load_content(\"speeches\", specific states, specific years)\n",
    "#Output:\n",
    "#   Dictionary and Transcript\n",
    "#   If no matches exist does not exist in database, returns []\n",
    "def get_hearing_transcript(hid, bid, speeches=None):\n",
    "  PID_IDX=1\n",
    "  BID_IDX=4\n",
    "  HID_IDX=3\n",
    "  VID_START_IDX = 9\n",
    "  VID_END_IDX = 10\n",
    "  LAST_NAME_IDX = 14\n",
    "  FIRST_NAME_IDX = 15\n",
    "  TEXT_IDX = 16\n",
    "  STARTING_TIME_IDX = 11\n",
    "  if speeches is None:\n",
    "    speeches=load_content(\"speeches\")\n",
    "\n",
    "  hid=str(hid)\n",
    "\n",
    "  lines = []\n",
    "  for row in speeches['rows']:\n",
    "    if hid == row[HID_IDX] and bid == row[BID_IDX]:\n",
    "      offset_time = add_seconds(\"00:00:00\", int(row[STARTING_TIME_IDX]))\n",
    "      tokenized_text = nltk.word_tokenize(row[TEXT_IDX])\n",
    "      sid = SentimentIntensityAnalyzer()\n",
    "      line = {'video start':row[VID_START_IDX],'video end':row[VID_END_IDX],'offset':offset_time,'bid':row[BID_IDX],\n",
    "              'first name':row[FIRST_NAME_IDX],'last name':row[LAST_NAME_IDX],'pid':row[PID_IDX],'text':row[TEXT_IDX], 'tokenized_text': tokenized_text, 'pos_tags': nltk.pos_tag(tokenized_text), 'sentiment': sid.polarity_scores(row[TEXT_IDX])}\n",
    "      lines.append(line)\n",
    "  return lines\n",
    "\n",
    "#Helper function to get bill discussion metadata\n",
    "def bill_discussion_info(hid, bid, hearings=None, speeches=None, videos=None):\n",
    "  return {\"metadata\":get_metadata_hearing(hid, hearings,videos), \"transcript\":get_hearing_transcript(hid, bid, speeches)}\n",
    "\n",
    "\n",
    "# accepts the output from get_hearing_transcript() and prints a transceript.\n",
    "def pprint_discussion(metadata, transcript_info):\n",
    "  videos = {}\n",
    "  for vid, url in metadata['videos']:\n",
    "    videos[vid] = url\n",
    "\n",
    "  print()\n",
    "  print(f\"] Discussion of {metadata['state']} {metadata['cname']} held on {metadata['hearing_date']}\")\n",
    "  print(f\"] {len(videos.keys())} videos\")\n",
    "  print(\"] printing transcript: \")\n",
    "\n",
    "  prev_video = -1\n",
    "  for line in transcript_info:\n",
    "    video = line['video start']\n",
    "    if video != prev_video:\n",
    "      print()\n",
    "      print(f\"] Discussing {line['bid']}\")\n",
    "      print(f\"] Video: {videos[line['video start']] if line['video start'] in videos else line['video start']}\")\n",
    "      print()\n",
    "      prev_video = video\n",
    "    print(f\"[{line['offset']}] {line['first name']} {line['last name']}: \")\n",
    "    print(f\"\\t{line['text']}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion = bill_discussion_info(53120, \"FL_20170HB883\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(discussion['transcript'][0]['text'])\n",
    "for item in discussion['transcript']:\n",
    "  print(f\"{item['text']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTaggedOutputs(discussion):\n",
    "    tagged_outputs = []\n",
    "    for item in discussion['transcript']:\n",
    "        print(f\"{item['text']}\\n\\n\")\n",
    "        ner_results = nlp(item['text'])\n",
    "        tagged_outputs.append(ner_results)  \n",
    "    return tagged_outputs\n",
    "\n",
    "\n",
    "def merge_org_entities(ner_output):\n",
    "    merged_entities = []\n",
    "    current_org = None\n",
    "\n",
    "    for entry in ner_output:\n",
    "        if entry['entity'] == 'B-ORG':\n",
    "            # Start a new organization\n",
    "            if current_org:\n",
    "                merged_entities.append(current_org)\n",
    "            current_org = {\n",
    "                'entity': 'ORG',\n",
    "                'score': entry['score'],\n",
    "                'word': entry['word'],\n",
    "                'start': entry['start'],\n",
    "                'end': entry['end']\n",
    "            }\n",
    "        elif entry['entity'] == 'I-ORG' and current_org:\n",
    "            # Append to the current organization\n",
    "            current_org['word'] += f\" {entry['word']}\"\n",
    "            current_org['score'] = min(current_org['score'], entry['score'])  # Take the minimum score\n",
    "            current_org['end'] = entry['end']\n",
    "        else:\n",
    "            # Finalize the current organization if it exists\n",
    "            if current_org:\n",
    "                merged_entities.append(current_org)\n",
    "                current_org = None\n",
    "            # Add non-ORG entities directly\n",
    "            merged_entities.append(entry)\n",
    "    \n",
    "    # Add the last organization if it's not already added\n",
    "    if current_org:\n",
    "        merged_entities.append(current_org)\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from Levenshtein import distance as levenshtein_distance  # Install with: pip install python-Levenshtein\n",
    "\n",
    "def validate_org_entities(output_list, csv_file_path):\n",
    "    validated_texts = []\n",
    "\n",
    "    # Read the CSV file values\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        csv_values = [row[1] for row in csv.reader(file)]  # Assuming single-column CSV\n",
    "    \n",
    "    # Iterate over model outputs\n",
    "    for entry in output_list:\n",
    "        for entity in merge_org_entities(entry):\n",
    "            entity_type = entity['entity']\n",
    "            text = entity['word']\n",
    "\n",
    "            #Check if ORG, concatenated B- and I- ORG above\n",
    "            if entity_type == \"ORG\":\n",
    "                # Compare the text to each value in the CSV file\n",
    "                print(text)\n",
    "                for csv_value in csv_values:\n",
    "                    if levenshtein_distance(text, csv_value) < 2:\n",
    "                        validated_texts.append(text)\n",
    "                        break  # Avoid duplicates if text matches multiple CSV values\n",
    "\n",
    "    return validated_texts\n",
    "\n",
    "\n",
    "csv_path = 'organizations.csv'  # Path to your CSV file\n",
    "\n",
    "data = [['FL_20170HB1209', '53253', '2017', '2017-03-14'],\n",
    " ['FL_20170HB1203', '53202', '2017', '2017-03-21'],\n",
    " ['FL_20170SB1206', '52788', '2017', '2017-03-27'],\n",
    " ['FL_20170HB1209', '53154', '2017', '2017-03-29'],\n",
    " ['FL_20170HB1205', '53106', '2017', '2017-04-06'],\n",
    " ['FL_20170HB1203', '53104', '2017', '2017-04-13'],\n",
    " ['FL_20170HB1209', '53089', '2017', '2017-04-20'],\n",
    " ['FL_20170SB1206', '53015', '2017', '2017-04-25'],\n",
    " ['FL_20170HB1203', '52820', '2017', '2017-04-26'],\n",
    " ['FL_20170HB1201', '53021', '2017', '2017-04-27'],\n",
    " ['FL_20170HB1203', '53059', '2017', '2017-05-02'],\n",
    " ['FL_20170SB1208', '253077', '2017', '2018-01-16'],\n",
    " ['FL_20170SB1120', '253154', '2017', '2018-01-18'],\n",
    " ['FL_20170HB1201', '253220', '2017', '2018-01-24'],\n",
    " ['FL_20180SB1200', '253400', '2017', '2018-02-06'],\n",
    " ['FL_20180HB1201', '253416', '2017', '2018-02-07'],\n",
    " ['FL_20180SB1200', '253507', '2017', '2018-02-14'],\n",
    " ['FL_20180HB1201', '253520', '2017', '2018-02-15'],\n",
    " ['FL_20180HB1201', '254054', '2017', '2018-03-01'],\n",
    " ['FL_20180HB1201', '254138', '2017', '2018-03-08'],\n",
    " ['FL_20180HB1201', '254432', '2017', '2018-03-09']]\n",
    "\n",
    "for entry in data:\n",
    "  bid = entry[0]\n",
    "  hid = entry[1]\n",
    "  discussion = bill_discussion_info(hid, bid)\n",
    "  tagged_outputs = createTaggedOutputs(discussion)\n",
    "  validated_texts = validate_org_entities(tagged_outputs, \"orgConcepts.csv\")\n",
    "  if validated_texts:\n",
    "      print(tagged_outputs)\n",
    "  print(\"Validated Texts:\", validated_texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
